{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyOKZVo6HqQh3WI8MG5sqBiJ",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU",
    "gpuClass": "standard"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/SMBH-1/tbd/blob/main/RNN_Projects_NLP.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Recurrent Neural Networks - used for NLP\n",
        "\n",
        "Much more capable of processing sequential data such as text or characters. Use to do the following:\n",
        "1) Sentiment Analysis\n",
        "2) Character Generation\n",
        "\n",
        "Sequence Data\n",
        "Unlike images, sequence data (i.e. long chains of text, weather patterns, videos & anything where notion of step or time is relevant needs to be processed & handled in special way)\n",
        "\n",
        "In textual data, need to keep track of order of characters/words. Can't simply encode entire paragraph into one data point wouldn't work. \n",
        "\n",
        "A) Bag of Words Method - every single unique word in dataset (vocabulary), placed in a dict w/ a number assigned as value. You keep track of frequency of words but lose ordering of the words. May work for simpler examples but order and multiple meanings of words depending on order make this a poor choice.\n",
        "\n",
        "B) Word Embedding - classifies or translates each word into a vector grouping similar words near each other as vector representations. Attempts to encode order of words, frequence of words, & meaning of words.\n",
        "\n",
        "Word embeddings learned by looking at many different training examples. Can add an embedding layer to beginning of model & while model trains, embedding layer will learn correct embedding for words. Can also use pretrained layers (like base layers for CNN)\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "id": "9McMuw5x-JV8"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Recurrent Neural Networks (RNNs) - up until now, we've used feed-forward neural nets. All data is fed forwards (all at once) from left to right thru network. Doesn't work well for text processing. \n",
        "  \n",
        "\n",
        "*   We read words left to right & keep track of current meaning of sentence so we can understand next word.\n",
        "\n",
        "This is what RNN is designed to do. It's a network that contains a LOOP. RNN processes one word at a time while maintaining an internal memory of what it's already seen. Allows it to treat words differently based on order in a sentence & slowly build understanding of entire input one word at a time.\n",
        "\n",
        "\n",
        "A single layer is called simpleRNN. It struggles with longer texts. \n",
        "\n",
        "LSTM (Long Short-Term Memory) - other recurrent layers that work better. LSTM is one example. Adds a way to access inputs from any timestamp in the past. In simpleRNN layer, input from previous timestamps gradually disappeared as we get further through input. \n",
        "\n",
        "LSTM - have long-term memory data structure storing all previously seen inputs as well as when we saw them. Allows us to access any previous value we want at any point in time. Adds to network complexity & allows it to discover more useful relationships between inputs & when they appear.\n"
      ],
      "metadata": {
        "id": "TY9LUk97NzAE"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Sentiment Analysis\n",
        "\n",
        "-Use RNNs\n",
        "-Process of computationally identifying and categorizing opinions expressed in piece of text, especially in order to determine whether writer's attitude towards a particular topic, product, etc. is positive, neutral or negative"
      ],
      "metadata": {
        "id": "FLhMXAG3RtlM"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "r7TmFhi3-Bcv",
        "outputId": "1ee345c2-605a-4044-8088-bdd34610cf0c"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Colab only includes TensorFlow 2.x; %tensorflow_version has no effect.\n",
            "Downloading data from https://storage.googleapis.com/tensorflow/tf-keras-datasets/imdb.npz\n",
            "17464789/17464789 [==============================] - 2s 0us/step\n"
          ]
        }
      ],
      "source": [
        "%tensorflow_version 2.x\n",
        "from keras.datasets import imdb\n",
        "from keras.preprocessing import sequence\n",
        "import tensorflow as tf\n",
        "import os\n",
        "import numpy as np\n",
        "\n",
        "VOCAB_SIZE = 88584\n",
        "\n",
        "MAXLEN = 250\n",
        "BATCH_SIZE = 64\n",
        "\n",
        "(train_data, train_labels), (test_data, test_labels) = imdb.load_data(num_words = VOCAB_SIZE)"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "train_data[0] #Looking at first review"
      ],
      "metadata": {
        "id": "NkEtRGtPSfmG"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Preprocessing (cont'd) - loaded reviews are of different lengths; this is an issue. We can't pass different length data in our neural network (like how we had to resize images for CNN). Make every review same length. \n",
        "\n",
        "\n",
        "*   If review is > 250 words, trime off extra words\n",
        "*   If review < 250 words, add necessary amount of 0's to make it equal to 250\n",
        "\n"
      ],
      "metadata": {
        "id": "WBYPeZRmTDSj"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "train_data = tf.keras.utils.pad_sequences(train_data, MAXLEN)\n",
        "test_data = tf.keras.utils.pad_sequences(test_data, MAXLEN)\n",
        "# train_data[1] - check to see how padding was done"
      ],
      "metadata": {
        "id": "Cr863zcrTbZQ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#Create model - use word embedding layer as first layer & add LSTM layer afterwards that feeds into dense node \n",
        "#to get our predicted sentiment; 32 stands for output dimension of vectors generated by embedding layer. can change if we want\n",
        "\n",
        "model = tf.keras.Sequential([\n",
        "    tf.keras.layers.Embedding(VOCAB_SIZE, 32),\n",
        "    tf.keras.layers.LSTM(32),\n",
        "    tf.keras.layers.Dense(1, activation='sigmoid')\n",
        "])\n",
        "\n",
        "model.summary()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "MGY7fHTzUp0A",
        "outputId": "df833833-1dc7-4c5e-e0de-68fcf55a4be8"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Model: \"sequential\"\n",
            "_________________________________________________________________\n",
            " Layer (type)                Output Shape              Param #   \n",
            "=================================================================\n",
            " embedding (Embedding)       (None, None, 32)          2834688   \n",
            "                                                                 \n",
            " lstm (LSTM)                 (None, 32)                8320      \n",
            "                                                                 \n",
            " dense (Dense)               (None, 1)                 33        \n",
            "                                                                 \n",
            "=================================================================\n",
            "Total params: 2,843,041\n",
            "Trainable params: 2,843,041\n",
            "Non-trainable params: 0\n",
            "_________________________________________________________________\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#Now to train above model\n",
        "\n",
        "model.compile(loss='binary_crossentropy', optimizer='rmsprop', metrics=['acc'])\n",
        "history = model.fit(train_data, train_labels, epochs=10, validation_split=0.2)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "S8WZAllGVgg8",
        "outputId": "e9aa5a68-f73f-4960-e46a-c0136474b16d"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 1/10\n",
            "625/625 [==============================] - 14s 12ms/step - loss: 0.4156 - acc: 0.8094 - val_loss: 0.2957 - val_acc: 0.8802\n",
            "Epoch 2/10\n",
            "625/625 [==============================] - 8s 13ms/step - loss: 0.2369 - acc: 0.9115 - val_loss: 0.2682 - val_acc: 0.8930\n",
            "Epoch 3/10\n",
            "625/625 [==============================] - 7s 11ms/step - loss: 0.1837 - acc: 0.9305 - val_loss: 0.2803 - val_acc: 0.8928\n",
            "Epoch 4/10\n",
            "625/625 [==============================] - 7s 12ms/step - loss: 0.1509 - acc: 0.9475 - val_loss: 0.3183 - val_acc: 0.8794\n",
            "Epoch 5/10\n",
            "625/625 [==============================] - 7s 12ms/step - loss: 0.1293 - acc: 0.9546 - val_loss: 0.3002 - val_acc: 0.8918\n",
            "Epoch 6/10\n",
            "625/625 [==============================] - 7s 12ms/step - loss: 0.1118 - acc: 0.9604 - val_loss: 0.3085 - val_acc: 0.8786\n",
            "Epoch 7/10\n",
            "625/625 [==============================] - 7s 12ms/step - loss: 0.0979 - acc: 0.9663 - val_loss: 0.3736 - val_acc: 0.8792\n",
            "Epoch 8/10\n",
            "625/625 [==============================] - 7s 12ms/step - loss: 0.0865 - acc: 0.9711 - val_loss: 0.4042 - val_acc: 0.8796\n",
            "Epoch 9/10\n",
            "625/625 [==============================] - 7s 12ms/step - loss: 0.0772 - acc: 0.9753 - val_loss: 0.3452 - val_acc: 0.8900\n",
            "Epoch 10/10\n",
            "625/625 [==============================] - 8s 13ms/step - loss: 0.0667 - acc: 0.9783 - val_loss: 0.4289 - val_acc: 0.8810\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "results = model.evaluate(test_data, test_labels)\n",
        "print(results)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "vFxLoGWqW9V5",
        "outputId": "2cc11f14-dfbd-4562-96fb-57b4e8b4509a"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "782/782 [==============================] - 4s 6ms/step - loss: 0.5649 - acc: 0.8435\n",
            "[0.5649113655090332, 0.8435199856758118]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#Predictions - with trained model, we make predictions on our reviews. Need to convert any review that we write into form\n",
        "#so network can understand it. To do that well, load encodings from dataset & use them to encode our own data\n",
        "\n",
        "word_index = imdb.get_word_index()\n",
        "\n",
        "def encode_text(text):\n",
        "  tokens = tf.keras.preprocessing.text.text_to_word_sequence(text)\n",
        "  tokens = [word_index[word] if word in word_index else 0 for word in tokens]\n",
        "  return tf.keras.utils.pad_sequences([tokens], MAXLEN)[0]\n",
        "\n",
        "text = 'that movie was just amazing, so amazing'\n",
        "encoded = encode_text(text)\n",
        "print(encoded)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "pBeS8QS_XX12",
        "outputId": "df99219f-15c4-425e-b0ae-4d2a6567dae6"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[  0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0\n",
            "   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0\n",
            "   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0\n",
            "   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0\n",
            "   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0\n",
            "   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0\n",
            "   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0\n",
            "   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0\n",
            "   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0\n",
            "   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0\n",
            "   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0\n",
            "   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0\n",
            "   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0\n",
            "   0   0   0   0   0   0   0   0   0  12  17  13  40 477  35 477]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#Decode function takes in movie review in int form (like above list) & returns text value\n",
        "\n",
        "reverse_word_index = {value: key for (key, value) in word_index.items()}\n",
        "\n",
        "def decode_integers(integers):\n",
        "  PAD = 0\n",
        "  text = ''\n",
        "  for num in integers:\n",
        "    if num!= PAD:\n",
        "      text += reverse_word_index[num] + ' '\n",
        "  return text[:-1]\n",
        "\n",
        "print(decode_integers(encoded))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "oyhFp74tZNcY",
        "outputId": "d520b253-ea62-4017-a90b-f78badb8b815"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "that movie was just amazing so amazing\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#Now to make prediction\n",
        "\n",
        "def predict(text):\n",
        "  encoded_text = encode_text(text)\n",
        "  pred = np.zeros((1,250))\n",
        "  pred[0] = encoded_text\n",
        "  result = model.predict(pred)\n",
        "  print(result[0])\n",
        "\n",
        "positive_review = \"That movie was so awesome! I really loved it and would watch it again because it was amazingly great\"\n",
        "predict(positive_review)\n",
        "\n",
        "negative_review = \"that movie sucked. I hated it and wouldn't watch it again. Was one of the worst things I've ever watched\"\n",
        "predict(negative_review)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "CWUagAiQZweO",
        "outputId": "9361692a-e985-4838-a7c5-c49bb1b9702d"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "1/1 [==============================] - 0s 19ms/step\n",
            "[0.8077212]\n",
            "1/1 [==============================] - 0s 17ms/step\n",
            "[0.21099128]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Using RNN to generate text\n",
        "\n",
        "Will show RNN an example of something we want it to recreate & it will learn how to write a version of it on its own. Done by using character predictive model that will take as input a variable length sequence & predict next character. Can use model many times in a row w/ output from last prediction as input for next call to generate a sequence."
      ],
      "metadata": {
        "id": "r-buJdEfcOnx"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "%tensorflow_version 2.x\n",
        "from keras.preprocessing import sequence\n",
        "import keras\n",
        "import tensorflow as tf\n",
        "import os\n",
        "import numpy as np\n",
        "\n",
        "path_to_file = tf.keras.utils.get_file('shakespeare.txt', 'https://storage.googleapis.com/download.tensorflow.org/data/shakespeare.txt')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "OMqBja8yciVg",
        "outputId": "6bcb79cd-1410-41e1-8ed6-3bbe58a45f4e"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Colab only includes TensorFlow 2.x; %tensorflow_version has no effect.\n",
            "Downloading data from https://storage.googleapis.com/download.tensorflow.org/data/shakespeare.txt\n",
            "1115394/1115394 [==============================] - 0s 0us/step\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from google.colab import files\n",
        "path_to_file = list(files.upload().keys())[0] #Running this code allows for us to choose own txt files on our own computer"
      ],
      "metadata": {
        "id": "NM717f71dKvc"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#Read, then decode for py2 compat\n",
        "text = open(path_to_file, 'rb').read().decode(encoding='utf-8')\n",
        "\n",
        "#Length of text is number of characters in it\n",
        "print('Length of text: {} characters'.format(len(text)))\n",
        "\n",
        "#Look at first 250 chars of text\n",
        "print(text[:250])"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "EHIZELcHdfq3",
        "outputId": "6802f962-9747-48b5-ffa5-40d65264ce92"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Length of text: 1115394 characters\n",
            "First Citizen:\n",
            "Before we proceed any further, hear me speak.\n",
            "\n",
            "All:\n",
            "Speak, speak.\n",
            "\n",
            "First Citizen:\n",
            "You are all resolved rather to die than to famish?\n",
            "\n",
            "All:\n",
            "Resolved. resolved.\n",
            "\n",
            "First Citizen:\n",
            "First, you know Caius Marcius is chief enemy to the people.\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#Now for encoding all this above txt file\n",
        "\n",
        "vocab = sorted(set(text)) #Sorts all unique characters in text\n",
        "\n",
        "#Creating a mapping from unique chars to indices\n",
        "char2idx = {u:i for i,u in enumerate(vocab)} #goes from letter/char to index\n",
        "idx2char = np.array(vocab) #reverse mapping going from index to letter/char\n",
        "\n",
        "def text_to_int(text):\n",
        "  return np.array([char2idx[c] for c in text]) #Convert every char in text to integer representation by pointing to char2idx dict\n",
        "\n",
        "text_as_int = text_to_int(text) #Convert entire txt file loaded above into int representation using function text_to_int()\n",
        "\n",
        "#Look at how part of our text is encoded now\n",
        "print('Text: ', text[:13])\n",
        "print('Encoded: ', text_to_int(text[:13]))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "7SeNcytQd-iX",
        "outputId": "ad917752-1dd8-4ed4-e44a-49ec81eb4516"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Text:  First Citizen\n",
            "Encoded:  [18 47 56 57 58  1 15 47 58 47 64 43 52]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#Function converts int list/array into text (reverses above function)\n",
        "\n",
        "def int_to_text(ints):\n",
        "  try:\n",
        "    ints = ints.numpy()\n",
        "  except:\n",
        "    pass\n",
        "  return ''.join(idx2char[ints])\n",
        "\n",
        "print(int_to_text(text_as_int[:13]))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "hZekwd_ufkGa",
        "outputId": "3f0af1b8-f143-46b4-fcef-f65cf9cd813b"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "First Citizen\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "\"\"\"Now to create training examples; goal is to feed model a sequence & have it return to us next char. \n",
        "Need to split our text data into many shorter sequences to pass to model as training examples.\n",
        "Training examples will use seq_length sequence as input & seq_length sequence as output where that sequence\n",
        "is original sequence shifted one letter to right \n",
        "\n",
        "ex: input: Hell | output: ello\n",
        "\"\"\"\n",
        "\n",
        "seq_length = 100 #length of sequence for a training example\n",
        "examples_per_epoch = len(text)//(seq_length+1) #if we are going w/ 100 chars length; need 101 chars in denominator\n",
        "\n",
        "#Create training examples/targets\n",
        "char_dataset = tf.data.Dataset.from_tensor_slices(text_as_int) #Will have 1+ million chars\n",
        "\n",
        "#Now can use batch method to turn stream of chars into batches of desired length (ex: 101 and dropping remainder chars)\n",
        "sequences = char_dataset.batch(seq_length+1, drop_remainder=True)"
      ],
      "metadata": {
        "id": "Fh6z3Cdpf_pb"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#Now take sequences of len 101 and split into input & output\n",
        "\n",
        "def split_input_target(chunk): #for example: hello\n",
        "  input_text = chunk[:-1] #hell\n",
        "  target_text = chunk[1:] #ello\n",
        "  return input_text, target_text #hell, ello\n",
        "\n",
        "dataset = sequences.map(split_input_target) #we use map to apply above function to every entry\n",
        "\n",
        "for x, y in dataset.take(2):\n",
        "  print(\"\\n\\nEXAMPLE\\n\")\n",
        "  print(\"INPUT\")\n",
        "  print(int_to_text(x))\n",
        "  print(\"\\nOUTPUT\")\n",
        "  print(int_to_text(y))\n",
        "\n",
        "#Make training batches\n",
        "BATCH_SIZE = 64\n",
        "VOCAB_SIZE = len(vocab) #Number of unique chars\n",
        "EMBEDDING_DIM = 256\n",
        "RNN_UNITS = 1024\n",
        "\n",
        "#Buffer size to shuffle dataset\n",
        "#(TF data is designed to work w/ possibly infinite seuqneces, so it doesn't\n",
        "#attempt to shuffle entire sequence in memory. Instead, it maintains a buffer\n",
        "# in which it shuffles elements)\n",
        "\n",
        "BUFFER_SIZE = 10000\n",
        "\n",
        "data = dataset.shuffle(BUFFER_SIZE).batch(BATCH_SIZE, drop_remainder=True)"
      ],
      "metadata": {
        "id": "W0Ki5Rldhh70"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#Building Model\n",
        "#Embedding layer, LSTM and one dense layer that contains a node for each unique char in training data\n",
        "#Dense layer will give probability distribution over all nodes\n",
        "\n",
        "def build_model(vocab_size, embedding_dim, rnn_units, batch_size):\n",
        "  model = tf.keras.Sequential([\n",
        "      tf.keras.layers.Embedding(vocab_size, embedding_dim,\n",
        "                                batch_input_shape=[batch_size, None]),\n",
        "      tf.keras.layers.LSTM(rnn_units,\n",
        "                           return_sequences = True,\n",
        "                           stateful=True,\n",
        "                           recurrent_initializer = 'glorot_uniform'),\n",
        "      tf.keras.layers.Dense(vocab_size)                               \n",
        "  ])\n",
        "  return model\n",
        "\n",
        "model = build_model(VOCAB_SIZE, EMBEDDING_DIM, RNN_UNITS, BATCH_SIZE)\n",
        "model.summary()\n",
        "\n",
        "#Model takes 64 training examples and gives us 64 outputs. We will rebuild \n",
        "#with same params we've saved & trained for model but only for 1 batch size to\n",
        "#get 1 prediction for 1 input sequence"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "E9C3yFKvjM3k",
        "outputId": "70b10a48-f45e-44c4-99d6-3f785dd19feb"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Model: \"sequential_1\"\n",
            "_________________________________________________________________\n",
            " Layer (type)                Output Shape              Param #   \n",
            "=================================================================\n",
            " embedding_1 (Embedding)     (64, None, 256)           16640     \n",
            "                                                                 \n",
            " lstm_1 (LSTM)               (64, None, 1024)          5246976   \n",
            "                                                                 \n",
            " dense_1 (Dense)             (64, None, 65)            66625     \n",
            "                                                                 \n",
            "=================================================================\n",
            "Total params: 5,330,241\n",
            "Trainable params: 5,330,241\n",
            "Non-trainable params: 0\n",
            "_________________________________________________________________\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#Creating Loss Function - create own loss function because our model will output\n",
        "#a (64, sequence_length, 65) shaped tensor that represents probability distribution\n",
        "#of each char at each timestep for every sequence in batch\n",
        "\n",
        "for input_example_batch, target_example_batch in data.take(1):\n",
        "  example_batch_predictions = model(input_example_batch) #ask our model for a prediction on our first batch of training data\n",
        "  print(example_batch_predictions.shape, \"# (batch_size, sequence_length, vocab_size)\") #print out output shape\n",
        "\n",
        "#We can see that prediction is an array of 64 arrays, one for each batch entry\n",
        "print(len(example_batch_predictions))\n",
        "print(example_batch_predictions)"
      ],
      "metadata": {
        "id": "Ea_ODvZnn2ay"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#Examine one prediction\n",
        "pred = example_batch_predictions[0]\n",
        "\n",
        "print(len(pred))\n",
        "print(pred)\n",
        "#Notice this is 2D array of len 100, where each interior array is prediction for next char at each time step"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "4YKfK-nSpayH",
        "outputId": "ff26881c-ff17-4b81-98e4-ab81c40280b0"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "100\n",
            "tf.Tensor(\n",
            "[[ 0.00155463  0.00232505 -0.00082527 ... -0.00340165  0.00014181\n",
            "   0.0018117 ]\n",
            " [-0.00400441 -0.00133221 -0.00264353 ... -0.01010216  0.00291852\n",
            "   0.00063826]\n",
            " [-0.00336853  0.00187674 -0.00252856 ... -0.00758613  0.00096282\n",
            "   0.00151361]\n",
            " ...\n",
            " [-0.00185623 -0.00531099 -0.00048059 ...  0.00311475  0.00249094\n",
            "   0.00244514]\n",
            " [-0.00201645 -0.00772558 -0.00131663 ... -0.00173657  0.00531403\n",
            "  -0.00062594]\n",
            " [-0.00915895 -0.01009805  0.00059497 ...  0.00081316  0.00036151\n",
            "   0.00430956]], shape=(100, 65), dtype=float32)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#We'll lastly look at prediction at first time step\n",
        "time_pred = pred[0]\n",
        "print(len(time_pred))\n",
        "print(time_pred)\n",
        "#its 65 values representing probability of each character occuring next"
      ],
      "metadata": {
        "id": "uI6-M_YTptBm"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#Determine predicted char we need to sample output distribution (pick a value based on probability)\n",
        "sampled_indices = tf.random.categorical(pred, num_samples = 1)\n",
        "\n",
        "#Next reshape array & convert all ints to nums to see actual chars\n",
        "sampled_indices = np.reshape(sampled_indices, (1,-1))[0]\n",
        "predicted_chars = int_to_text(sampled_indices)\n",
        "\n",
        "predicted_chars #this is what model predicted for training sequence 1\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 35
        },
        "id": "kmWIAY1DqFtU",
        "outputId": "d0fc8ab1-4231-4ac2-e78f-71e66aa69b4b"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "\",TysicjQ.:VI3kk&r&kY&xqg\\nGmRUKohpvBDEoqwcQBmDqAvjwzyD\\n.WlmAxKJ:?k' s$ghJkZaPawbmOmmOiSQCGR$cSD DskGF\""
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            }
          },
          "metadata": {},
          "execution_count": 32
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#Create loss function (keras built in example below) that can compare output to expected output & give us some numeric value\n",
        "#representing how close the two were\n",
        "\n",
        "def loss(labels, logits):\n",
        "  return tf.keras.losses.sparse_categorical_crossentropy(labels, logits, from_logits = True)"
      ],
      "metadata": {
        "id": "CLpy44U1rETv"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#Compile model \n",
        "\n",
        "model.compile(optimizer='adam', loss=loss)"
      ],
      "metadata": {
        "id": "ut9zYi0BrXnb"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#Create checkpoints - gets model to save checkpoints as it trains; allows us to load model from a checkpoint & continue training it\n",
        "\n",
        "#Directory where checkpoints will be saved\n",
        "checkpoint_dir = './training_checkpoints'\n",
        "#Name of checkpoint files\n",
        "checkpoint_prefix = os.path.join(checkpoint_dir, 'ckpt_{epoch}')\n",
        "\n",
        "checkpoint_callback=tf.keras.callbacks.ModelCheckpoint(\n",
        "    filepath=checkpoint_prefix,\n",
        "    save_weights_only=True\n",
        ")"
      ],
      "metadata": {
        "id": "hur6TUdHr6uF"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#Train model\n",
        "\n",
        "history = model.fit(data, epochs=40, callbacks=[checkpoint_callback])"
      ],
      "metadata": {
        "id": "M5lIJCvwsgGa"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#Rebuild Model - we'll rebuild model from previous checkpoint using batch_size = 1 so we can feed one piece of text & have model make prediction\n",
        "\n",
        "model = build_model(VOCAB_SIZE, EMBEDDING_DIM, RNN_UNITS, batch_size=1)\n",
        "\n",
        "model.load_weights(tf.train.latest_checkpoint(checkpoint_dir))\n",
        "model.build(tf.TensorShape([1, None]))\n",
        "\n",
        "checkpoint_num = 40\n",
        "model.load_weights(tf.train.load_checkpoint(\"./training_checkpoints/ckpt_\" + str(checkpoint_num)))\n",
        "model.build(tf.TensorShape([1, None]))"
      ],
      "metadata": {
        "id": "Yq2QofJGtxdK"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#Generate Text using below function\n",
        "\n",
        "def generate_text(model, start_string):\n",
        "  #Evaluation step (generating text using learned model)\n",
        "\n",
        "  #Number of chars to generate\n",
        "  num_generate = 800\n",
        "\n",
        "  #Converting start str to num (vectorizing)\n",
        "  input_eval = [char2idx[s] for s in start_string]\n",
        "  input_eval = tf.expand_dims(input_eval, 0)\n",
        "\n",
        "  #Empty str to store our results\n",
        "  text_generated = []\n",
        "\n",
        "  #Low temp results in more predictable text.\n",
        "  #Higher temp = more surprising text.\n",
        "  #Experiment to find best setting\n",
        "  temperature = 1.0\n",
        "\n",
        "  #Here batch size == 1\n",
        "  model.reset_states()\n",
        "  for i in range(num_generate):\n",
        "    predictions = model(input_eval)\n",
        "    #remove the batch dimension\n",
        "\n",
        "    predictions = tf.squeeze(predictions, 0)\n",
        "\n",
        "    #Using categorial distribution to predict char returned by model\n",
        "    predictions = predictions / temperature\n",
        "    predicted_id = tf.random.categorical(predictions, num_samples=1)[-1,0].numpy()\n",
        "\n",
        "    #We pass predicted char as next input to model along w/ previous hidden state\n",
        "    input_eval = tf.expand_dims([predicted_id], 0)\n",
        "\n",
        "    text_generated.append(idx2char[predicted_id])\n",
        "\n",
        "  return (start_string + ''.join(text_generated))\n",
        "\n",
        "inp = input('Type a starting string: ')\n",
        "print(generate_text(model, inp))\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "vUJS_lRXumXz",
        "outputId": "f08ccdd5-44bb-49ab-f9fc-fef7c3549955"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Type a starting string: romeo\n",
            "romeo.\n",
            "\n",
            "FROTH:\n",
            "Ay, commend me to you. Thou hast sworn brother,\n",
            "Tull favourage dignities us, his fashions,\n",
            "They be noted in this conatin.\n",
            "Tell me, thou dost take quandave\n",
            "To use it justice, make I with this fame\n",
            "I' the insu take up down to pity of her,\n",
            "And we will schat the glasses. The temple to his study,\n",
            "Which strength itself in painted liberty\n",
            "That Edward you fither, whou forward our hope to tear,\n",
            "And left thee there and hed proclaim myself.\n",
            "Alack, for sour adversuisand! Tybalt dead, continue fathers! and do you grow proligy,\n",
            "Unless they are best received. But, O, how the\n",
            "dead king's art,\n",
            "Whethe leisure and thy father and\n",
            "Thy friends, mine honour, as he does it will;\n",
            "And weep against your brother, betimes,\n",
            "Lest in reverence I may break ite our charge for me;\n",
            "Or else you early not me; I charg\n"
          ]
        }
      ]
    }
  ]
}